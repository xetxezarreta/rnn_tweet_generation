{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0EJMRASTG8j"
   },
   "outputs": [],
   "source": [
    "# base\n",
    "import re, pickle\n",
    "import numpy as np\n",
    "\n",
    "# tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCH21BnXTG8m"
   },
   "outputs": [],
   "source": [
    "with open('data/raw_text.txt', 'r', encoding='utf8') as myfile:\n",
    "    raw_text = myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrigU3ZBTG8o"
   },
   "outputs": [],
   "source": [
    "def clean_tweet(t):\n",
    "    # remove quotes\n",
    "    t = re.sub(r'\"@.*', '', t)\n",
    "    t = re.sub(r'^“.*”$', '', t)\n",
    "    # remove URLs\n",
    "    t = re.sub(r'https*:\\/\\/\\S*', '', t)\n",
    "    t = re.sub(r'pic\\.twitter\\.com\\/\\S*', '', t)\n",
    "    # remove \\n\n",
    "    t = re.sub('\\n', ' ', t)\n",
    "    # remove extra whitespaces\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    # replace '&amp' with 'and'\n",
    "    t = re.sub('&amp;', 'and', t) \n",
    "    return(t)\n",
    "\n",
    "raw_text = clean_tweet(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hen23vS-TG8q"
   },
   "outputs": [],
   "source": [
    "def prepare_tweet_clf(t):\n",
    "    # clean\n",
    "    t = t.lower()\n",
    "    t = re.sub(\"'ll\", ' will', t) # replace abbreviations\n",
    "    t = re.sub(\"won't\", 'will not', t)\n",
    "    t = re.sub(\"n't\", ' not', t) \n",
    "    t = re.sub(r'@[A-Za-z0-9_]+', '', t) # remove @mention\n",
    "    t = re.sub(r'#[A-Za-z0-9_]+', '', t) # remove #tag\n",
    "    t = re.sub(r'[^a-zA-Z ]', '', t) # remove special characters\n",
    "    t = re.sub(\"\\s\\s+\", \" \", t) # remove multiple spaces \n",
    "    return t\n",
    "\n",
    "raw_text = prepare_tweet_clf(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7109,
     "status": "ok",
     "timestamp": 1591077322294,
     "user": {
      "displayName": "Xabier Etxezarreta Argarate",
      "photoUrl": "",
      "userId": "15184980590702807715"
     },
     "user_tz": -120
    },
    "id": "a7HVra7VTG8s",
    "outputId": "efc68273-b0a5-4985-88b2-26d710495c02"
   },
   "outputs": [],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 937,
     "status": "ok",
     "timestamp": 1591077323254,
     "user": {
      "displayName": "Xabier Etxezarreta Argarate",
      "photoUrl": "",
      "userId": "15184980590702807715"
     },
     "user_tz": -120
    },
    "id": "N9xlCGtkTG8v",
    "outputId": "2f85ef91-c3c7-43a5-9996-abc2066117b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  442943\n",
      "Total Vocab:  27\n"
     ]
    }
   ],
   "source": [
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCR3C_99TG8x"
   },
   "outputs": [],
   "source": [
    "with open('chars.txt', 'wb') as fp:\n",
    "    pickle.dump(chars, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vkXw6xLTG8z"
   },
   "outputs": [],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5943,
     "status": "ok",
     "timestamp": 1591077328272,
     "user": {
      "displayName": "Xabier Etxezarreta Argarate",
      "photoUrl": "",
      "userId": "15184980590702807715"
     },
     "user_tz": -120
    },
    "id": "8CSC5N4qTG81",
    "outputId": "a08acebc-d341-4252-9272-3ff461db63f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  398558\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Total Patterns: \", len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18838,
     "status": "ok",
     "timestamp": 1591077341174,
     "user": {
      "displayName": "Xabier Etxezarreta Argarate",
      "photoUrl": "",
      "userId": "15184980590702807715"
     },
     "user_tz": -120
    },
    "id": "nyJwelaMTG83",
    "outputId": "9b0c02ae-a1e0-4d31-9d3b-d52050300986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, None, 256)         264192    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 256)         525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 27)                6939      \n",
      "=================================================================\n",
      "Total params: 1,321,755\n",
      "Trainable params: 1,321,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(X_train, (len(X_train), seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(y_train)\n",
    "# define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(256, dropout=0.2, input_shape=(None, X.shape[2]), return_sequences=True),\n",
    "    LSTM(256, dropout=0.2, return_sequences=True),\n",
    "    LSTM(256, dropout=0.2),\n",
    "    Dense(y.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7361816,
     "status": "ok",
     "timestamp": 1591084684158,
     "user": {
      "displayName": "Xabier Etxezarreta Argarate",
      "photoUrl": "",
      "userId": "15184980590702807715"
     },
     "user_tz": -120
    },
    "id": "aLphD14yTG85",
    "outputId": "a9436edf-32d4-4def-9e71-094550e0306a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2803/2803 [==============================] - 124s 44ms/step - loss: 2.5124 - val_loss: 2.1875\n",
      "Epoch 2/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 2.0310 - val_loss: 1.8248\n",
      "Epoch 3/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.8358 - val_loss: 1.6758\n",
      "Epoch 4/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.7248 - val_loss: 1.5829\n",
      "Epoch 5/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.6510 - val_loss: 1.5273\n",
      "Epoch 6/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.5943 - val_loss: 1.4857\n",
      "Epoch 7/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.5487 - val_loss: 1.4597\n",
      "Epoch 8/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.5121 - val_loss: 1.4165\n",
      "Epoch 9/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.4818 - val_loss: 1.4814\n",
      "Epoch 10/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.4540 - val_loss: 1.3803\n",
      "Epoch 11/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.4312 - val_loss: 1.3950\n",
      "Epoch 12/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.4093 - val_loss: 1.3528\n",
      "Epoch 13/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.3928 - val_loss: 1.3645\n",
      "Epoch 14/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.3758 - val_loss: 1.3398\n",
      "Epoch 15/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.3618 - val_loss: 1.3221\n",
      "Epoch 16/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.3493 - val_loss: 1.3309\n",
      "Epoch 17/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.3352 - val_loss: 1.3190\n",
      "Epoch 18/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.3247 - val_loss: 1.3070\n",
      "Epoch 19/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.3137 - val_loss: 1.2954\n",
      "Epoch 20/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.3034 - val_loss: 1.2936\n",
      "Epoch 21/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.2947 - val_loss: 1.2989\n",
      "Epoch 22/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.2859 - val_loss: 1.2800\n",
      "Epoch 23/200\n",
      "2803/2803 [==============================] - 122s 44ms/step - loss: 1.2770 - val_loss: 1.3126\n",
      "Epoch 24/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2702 - val_loss: 1.3208\n",
      "Epoch 25/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2624 - val_loss: 1.2767\n",
      "Epoch 26/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2592 - val_loss: 1.2786\n",
      "Epoch 27/200\n",
      "2803/2803 [==============================] - 122s 43ms/step - loss: 1.2522 - val_loss: 1.3040\n",
      "Epoch 28/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2446 - val_loss: 1.2709\n",
      "Epoch 29/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2413 - val_loss: 1.2712\n",
      "Epoch 30/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2334 - val_loss: 1.2632\n",
      "Epoch 31/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2300 - val_loss: 1.2654\n",
      "Epoch 32/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2244 - val_loss: 1.2842\n",
      "Epoch 33/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2215 - val_loss: 1.2612\n",
      "Epoch 34/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2177 - val_loss: 1.2752\n",
      "Epoch 35/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2125 - val_loss: 1.2565\n",
      "Epoch 36/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2100 - val_loss: 1.2556\n",
      "Epoch 37/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.2056 - val_loss: 1.2577\n",
      "Epoch 38/200\n",
      "2803/2803 [==============================] - 122s 43ms/step - loss: 1.2033 - val_loss: 1.2587\n",
      "Epoch 39/200\n",
      "2803/2803 [==============================] - 122s 43ms/step - loss: 1.1986 - val_loss: 1.2585\n",
      "Epoch 40/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.1945 - val_loss: 1.2525\n",
      "Epoch 41/200\n",
      "2803/2803 [==============================] - 122s 43ms/step - loss: 1.1935 - val_loss: 1.2511\n",
      "Epoch 42/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.1890 - val_loss: 1.2516\n",
      "Epoch 43/200\n",
      "2803/2803 [==============================] - 122s 43ms/step - loss: 1.1878 - val_loss: 1.2492\n",
      "Epoch 44/200\n",
      "2803/2803 [==============================] - 122s 44ms/step - loss: 1.1857 - val_loss: 1.2506\n",
      "Epoch 45/200\n",
      "2803/2803 [==============================] - 122s 44ms/step - loss: 1.1804 - val_loss: 1.2529\n",
      "Epoch 46/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.1804 - val_loss: 1.2628\n",
      "Epoch 47/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.1751 - val_loss: 1.2459\n",
      "Epoch 48/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.1754 - val_loss: 1.2451\n",
      "Epoch 49/200\n",
      "2803/2803 [==============================] - 123s 44ms/step - loss: 1.1731 - val_loss: 1.2445\n",
      "Epoch 50/200\n",
      "2803/2803 [==============================] - 122s 43ms/step - loss: 1.1701 - val_loss: 1.2424\n",
      "Epoch 51/200\n",
      "2803/2803 [==============================] - 122s 43ms/step - loss: 1.1673 - val_loss: 1.2516\n",
      "Epoch 52/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.1664 - val_loss: 1.2453\n",
      "Epoch 53/200\n",
      "2803/2803 [==============================] - 122s 43ms/step - loss: 1.1638 - val_loss: 1.2411\n",
      "Epoch 54/200\n",
      "2803/2803 [==============================] - 122s 43ms/step - loss: 1.1640 - val_loss: 1.2470\n",
      "Epoch 55/200\n",
      "2803/2803 [==============================] - 122s 44ms/step - loss: 1.1607 - val_loss: 1.2556\n",
      "Epoch 56/200\n",
      "2803/2803 [==============================] - 122s 43ms/step - loss: 1.1592 - val_loss: 1.2611\n",
      "Epoch 57/200\n",
      "2803/2803 [==============================] - 122s 44ms/step - loss: 1.1569 - val_loss: 1.2436\n",
      "Epoch 58/200\n",
      "2803/2803 [==============================] - 122s 44ms/step - loss: 1.1549 - val_loss: 1.2443\n",
      "Epoch 59/200\n",
      "2803/2803 [==============================] - 122s 44ms/step - loss: 1.1531 - val_loss: 1.2602\n",
      "Epoch 60/200\n",
      "2803/2803 [==============================] - 121s 43ms/step - loss: 1.1522 - val_loss: 1.2453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd7602ab390>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = [EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')]\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=200, batch_size=128, callbacks=early_stop, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06AA1-5nTG86"
   },
   "outputs": [],
   "source": [
    "model.save('models/model_v1.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "fit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
