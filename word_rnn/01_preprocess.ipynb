{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_data(filename):\n",
    "    # open the file as read only\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        # read all text\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# clean text\n",
    "def clean_text(t):\n",
    "    # to lower\n",
    "    t = t.lower()\n",
    "    # remove quotes\n",
    "    t = re.sub(r'\"@.*', '', t)\n",
    "    t = re.sub(r'^“.*”$', '', t)\n",
    "    # remove URLs\n",
    "    t = re.sub(r'https*:\\/\\/\\S*', '', t)\n",
    "    t = re.sub(r'pic\\.twitter\\.com\\/\\S*', '', t)\n",
    "    # remove \\n\n",
    "    t = re.sub('\\n', ' ', t)\n",
    "    # remove extra whitespaces\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    # replace '&amp' with 'and'\n",
    "    t = re.sub('&amp;', 'and', t)     \n",
    "    # replace abbreviations\n",
    "    t = re.sub(\"'ll\", ' will', t)\n",
    "    t = re.sub(\"won't\", 'will not', t)\n",
    "    t = re.sub(\"n't\", ' not', t) \n",
    "    # remove @mention\n",
    "    t = re.sub(r'@[A-Za-z0-9_]+', '', t) \n",
    "    # remove #tag\n",
    "    t = re.sub(r'#[A-Za-z0-9_]+', '', t) \n",
    "    # remove special characters\n",
    "    t = re.sub(r'[^a-zA-Z ]', '', t) \n",
    "    # remove multiple spaces \n",
    "    t = re.sub(\"\\s\\s+\", \" \", t) \n",
    "    # split into tokens by white space\n",
    "    words = t.split()\n",
    "    return words\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unless', 'you', 'handle', 'snakes', 'speak', 'in', 'tongues', 'or', 'consider', 'donald', 'trump', 'the', 'second', 'son', 'of', 'god', 'your', 'christianity', 'does', 'not', 'count', 'for', 'maga', 'people', 'either', 'love', 'trump', 'or', 'hate', 'him', 'and', 'were', 'just', 'gonna', 'have', 'to', 'out', 'love', 'the', 'haters', 'on', 'november', 'rd', 'is', 'it', 'possible', 'that', 'president', 'trump', 'is', 'going', 'to', 'unleash', 'the', 'kraken', 'now', 'that', 'riots', 'have', 'exposed', 'the', 'enemy', 'he', 'will', 'not', 'have', 'to', 'be', 'so', 'politically', 'correct', 'its', 'wartime', 'arkansas', 'beverly', 'gop', 'hillbillies', 'made', 'own', 'trump', 'gop', 'hillbillies', 'of', 'hell', 'distroy', 'any', 'they', 'put', 'their', 'hoofs', 'on', 'will', 'stop', 'at', 'nothing', 'to', 'dad', 'was', 'terminaldid', 'not', 'knowhe', 'feared', 'i', 'was', 'passedthese', 'filth', 'put', 'my', 'wifes', 'name', 'my', 'property', 'on', 'date', 'site', 'you', 'were', 'not', 'aware', 'the', 'fast', 'food', 'ceo', 'of', 'these', 'three', 'companies', 'donated', 'k', 'to', 'trump', 'make', 'sure', 'to', 'cancel', 'all', 'three', 'not', 'just', 'wendys', 'hello', 'many', 'users', 'are', 'reporting', 'that', 'keyboard', 'predictions', 'including', 'mine', 'when', 'typing', 'vote', 'vote', 'with', 'space', 'or', 'vote', 'for', 'automaticly', 'detect', 'for', 'trump', 'even', 'though', 'ive', 'never', 'in', 'my', 'life', 'wrote', 'such', 'thing', 'also', 'im', 'not', 'a', 'us', 'citizen', 'pleae', 'explain', 'trump', 'just', 'said', 'hes', 'ready', 'to', 'deploy', 'the', 'military', 'nationally', 'if', 'he', 'needs', 'to', 'do', 'it', 'hes', 'already', 'dispatching']\n",
      "Total words: 226311\n",
      "Unique words: 18049\n",
      "Total Sequences: 226260\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'data/trump_raw_text.txt'\n",
    "doc = load_data(in_filename)\n",
    "#print(doc[:200])\n",
    "\n",
    "# clean document\n",
    "words = clean_text(doc)\n",
    "print(words[:200])\n",
    "print('Total words: %d' % len(words))\n",
    "print('Unique words: %d' % len(set(words)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(words)):\n",
    "    # select sequence of tokens\n",
    "    seq = words[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'data/trump_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
