{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_data(filename):\n",
    "    # open the file as read only\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        # read all text\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# clean text\n",
    "def clean_text(t):\n",
    "    # to lower\n",
    "    t = t.lower()\n",
    "    # remove quotes\n",
    "    t = re.sub(r'\"@.*', '', t)\n",
    "    t = re.sub(r'^“.*”$', '', t)\n",
    "    # remove URLs\n",
    "    t = re.sub(r'https*:\\/\\/\\S*', '', t)\n",
    "    t = re.sub(r'pic\\.twitter\\.com\\/\\S*', '', t)\n",
    "    # remove \\n\n",
    "    t = re.sub('\\n', ' ', t)\n",
    "    # remove extra whitespaces\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    # replace '&amp' with 'and'\n",
    "    t = re.sub('&amp;', 'and', t)     \n",
    "    # replace abbreviations\n",
    "    t = re.sub(\"'ll\", ' will', t)\n",
    "    t = re.sub(\"won't\", 'will not', t)\n",
    "    t = re.sub(\"n't\", ' not', t) \n",
    "    # remove @mention\n",
    "    t = re.sub(r'@[A-Za-z0-9_]+', '', t) \n",
    "    # remove #tag\n",
    "    t = re.sub(r'#[A-Za-z0-9_]+', '', t) \n",
    "    # remove special characters\n",
    "    t = re.sub(r'[^a-zA-Z ]', '', t) \n",
    "    # remove multiple spaces \n",
    "    t = re.sub(\"\\s\\s+\", \" \", t) \n",
    "    # split into tokens by white space\n",
    "    words = t.split()\n",
    "    return words\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', 'face', 'of', 'ongoing', 'nationwide', 'protests', 'trump', 'administration', 'deploys', 'military', 'to', 'the', 'capital', 'lights', 'that', 'usually', 'illuminate', 'the', 'white', 'house', 'have', 'been', 'shut', 'off', 'as', 'regime', 'leader', 'trump', 'takes', 'shelter', 'inside', 'im', 'not', 'trying', 'to', 'brag', 'about', 'my', 'skills', 'at', 'predicting', 'im', 'saying', 'that', 'what', 'trump', 'is', 'was', 'always', 'obvious', 'and', 'anybody', 'who', 'voted', 'for', 'him', 'should', 'be', 'understood', 'as', 'being', 'profascist', 'from', 'the', 'getgo', 'nobody', 'gets', 'to', 'say', 'they', 'did', 'not', 'know', 'trumps', 'comment', 'that', 'he', 'was', 'going', 'to', 'activate', 'bill', 'barr', 'makes', 'me', 'wonder', 'whether', 'our', 'attorney', 'general', 'when', 'his', 'staff', 'goes', 'home', 'on', 'friday', 'just', 'powers', 'down', 'at', 'his', 'desk', 'for', 'the', 'weekend', 'this', 'is', 'sad', 'and', 'nobody', 'is', 'doing', 'anything', 'about', 'this', 'so', 'heres', 'a', 'list', 'of', 'companies', 'supporting', 'trumps', 'reelection', 'bang', 'inn', 'n', 'out', 'chick', 'fil', 'a', 'taco', 'bell', 'mcdonalds', 'wendys', 'kfc', 'pizza', 'hut', 'olive', 'garden', 'waffle', 'house', 'ihop', 'carls', 'jrso', 'if', 'you', 'see', 'me', 'starting', 'to', 'get', 'thinner', 'and', 'toned', 'do', 'not', 'ask', 'me', 'why', 'law', 'order', 'in', 'philadelphia', 'now', 'they', 'are', 'looting', 'stores', 'call', 'in', 'our', 'great', 'national', 'guard', 'like', 'they', 'finally', 'did', 'thank', 'you', 'president', 'trump', 'last', 'night', 'in', 'minneapolis', 'is', 'this', 'what', 'voters', 'want', 'with', 'sleepy', 'joe', 'all', 'dems', 'its']\n",
      "Total words: 21016\n",
      "Unique words: 4097\n",
      "Total Sequences: 20965\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'data/raw_text.txt'\n",
    "doc = load_data(in_filename)\n",
    "#print(doc[:200])\n",
    "\n",
    "# clean document\n",
    "words = clean_text(doc)\n",
    "print(words[:200])\n",
    "print('Total words: %d' % len(words))\n",
    "print('Unique words: %d' % len(set(words)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(words)):\n",
    "    # select sequence of tokens\n",
    "    seq = words[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'data/trump_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
