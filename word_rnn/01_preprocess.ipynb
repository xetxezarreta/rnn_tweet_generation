{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_data(filename):\n",
    "    # open the file as read only\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        # read all text\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# clean text\n",
    "def clean_text(t):\n",
    "    # to lower\n",
    "    t = t.lower()\n",
    "    # remove quotes\n",
    "    t = re.sub(r'\"@.*', '', t)\n",
    "    t = re.sub(r'^“.*”$', '', t)\n",
    "    # remove URLs\n",
    "    t = re.sub(r'https*:\\/\\/\\S*', '', t)\n",
    "    t = re.sub(r'pic\\.twitter\\.com\\/\\S*', '', t)\n",
    "    # remove \\n\n",
    "    t = re.sub('\\n', ' ', t)\n",
    "    # remove extra whitespaces\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    # replace '&amp' with 'and'\n",
    "    t = re.sub('&amp;', 'and', t)     \n",
    "    # replace abbreviations\n",
    "    t = re.sub(\"'ll\", ' will', t)\n",
    "    t = re.sub(\"won't\", 'will not', t)\n",
    "    t = re.sub(\"n't\", ' not', t) \n",
    "    # remove @mention\n",
    "    t = re.sub(r'@[A-Za-z0-9_]+', '', t) \n",
    "    # remove #tag\n",
    "    t = re.sub(r'#[A-Za-z0-9_]+', '', t) \n",
    "    # remove special characters\n",
    "    t = re.sub(r'[^a-zA-Z ]', '', t) \n",
    "    # remove multiple spaces \n",
    "    t = re.sub(\"\\s\\s+\", \" \", t) \n",
    "    # split into tokens by white space\n",
    "    words = t.split()\n",
    "    return words\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'to', 'be', 'with', 'our', 'wonderful', 'men', 'and', 'women', 'of', 'the', 'what', 'a', 'job', 'they', 'are', 'doing', 'the', 'problem', 'is', 'not', 'the', 'very', 'talented', 'lowflying', 'helicopter', 'pilots', 'wanting', 'to', 'save', 'our', 'city', 'the', 'problem', 'is', 'the', 'arsonists', 'looters', 'criminals', 'and', 'anarchists', 'wanting', 'to', 'destroy', 'it', 'and', 'our', 'country', 'michael', 'is', 'tough', 'smart', 'and', 'loves', 'our', 'country', 'this', 'has', 'been', 'a', 'big', 'battle', 'in', 'congress', 'for', 'years', 'thank', 'you', 'to', 'our', 'great', 'republican', 'senate', 'congratulations', 'to', 'michael', 'pack', 'nobody', 'has', 'any', 'idea', 'what', 'a', 'big', 'victory', 'this', 'is', 'for', 'america', 'why', 'because', 'he', 'is', 'going', 'to', 'be', 'running', 'the', 'voice', 'of', 'america', 'and', 'everything', 'associated', 'with', 'it', 'to', 'the', 'united', 'states', 'we', 'have', 'now', 'brought', 'more', 'than', 'american', 'hostages', 'and', 'detainees', 'back', 'home', 'since', 'i', 'took', 'office', 'thank', 'you', 'to', 'iran', 'it', 'shows', 'a', 'deal', 'is', 'possible', 'i', 'just', 'got', 'off', 'the', 'phone', 'with', 'former', 'american', 'hostage', 'michael', 'white', 'who', 'is', 'now', 'in', 'zurich', 'after', 'being', 'released', 'from', 'iran', 'he', 'will', 'be', 'on', 'a', 'us', 'plane', 'shortly', 'and', 'is', 'coming', 'home', 'we', 'need', 'healing', 'but', 'we', 'also', 'need', 'strength', 'its', 'time', 'for', 'all', 'of', 'americas', 'governors', 'and', 'mayors', 'to', 'restore', 'law', 'and', 'order', 'president', 'provided', 'permanent', 'funding', 'to', 'historically', 'black', 'colleges']\n",
      "Total words: 164572\n",
      "Unique words: 10304\n",
      "Total Sequences: 164521\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'data/trump_raw_text.txt'\n",
    "doc = load_data(in_filename)\n",
    "#print(doc[:200])\n",
    "\n",
    "# clean document\n",
    "words = clean_text(doc)\n",
    "print(words[:200])\n",
    "print('Total words: %d' % len(words))\n",
    "print('Unique words: %d' % len(set(words)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(words)):\n",
    "    # select sequence of tokens\n",
    "    seq = words[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'data/trump_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
