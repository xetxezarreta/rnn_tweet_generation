{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "\n",
    "conf = SparkConf().setAppName('ITAPP-Preprocess').setMaster('spark://tf2:7077').set(\"spark.executor.memory\", \"5g\").set(\"spark.driver.memory\", \"5g\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"inferSchema\", \"true\").json('/home/ubuntu/export/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import re\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, HashingTF, IDF, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "def clean_text(t):\n",
    "    # to lower\n",
    "    t = t.lower()\n",
    "    # remove quotes\n",
    "    t = re.sub(r'\"@.*', '', t)\n",
    "    t = re.sub(r'^“.*”$', '', t)\n",
    "    # remove URLs\n",
    "    t = re.sub(r'https*:\\/\\/\\S*', '', t)\n",
    "    t = re.sub(r'pic\\.twitter\\.com\\/\\S*', '', t)\n",
    "    # remove \\n\n",
    "    t = re.sub('\\n', ' ', t)\n",
    "    # remove extra whitespaces\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    # replace '&amp' with 'and'\n",
    "    t = re.sub('&amp;', 'and', t)     \n",
    "    # replace abbreviations\n",
    "    t = re.sub(\"'ll\", ' will', t)\n",
    "    t = re.sub(\"won't\", 'will not', t)\n",
    "    t = re.sub(\"n't\", ' not', t) \n",
    "    # remove @mention\n",
    "    t = re.sub(r'@[A-Za-z0-9_]+', '', t) \n",
    "    # remove #tag\n",
    "    t = re.sub(r'#[A-Za-z0-9_]+', '', t) \n",
    "    # remove special characters\n",
    "    t = re.sub(r'[^a-zA-Z ]', '', t) \n",
    "    # remove multiple spaces \n",
    "    t = re.sub(\"\\s\\s+\", \" \", t) \n",
    "    return(t)\n",
    "\n",
    "cleaner = udf(lambda x : clean_text(x))\n",
    "toLower = udf(lambda x : str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_vocab(vocab, char):\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == char:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def inverse_lookup_vocab(vocab, int_val):\n",
    "    return vocab[int_val]\n",
    "\n",
    "\n",
    "def vocab_to_dict(vocab):\n",
    "    dict_v = {}\n",
    "    for i in range(len(vocab)):\n",
    "        dict_v[vocab[i]] = i\n",
    "    return dict_v\n",
    "\n",
    "def encode_arr(arr, dict_v):\n",
    "    res = []\n",
    "    for i in arr:\n",
    "        res.append(dict_v[i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_candidate(df, candidate=\"Trump\", dict_v=None):\n",
    "    df = df.select(\"Text\").where(\"Tag == '\"+ candidate + \"'\") \n",
    "    df = df.withColumn(\"Text\", toLower(df.Text))\n",
    "    # Remove duplicated tweets\n",
    "    df = df.select(\"Text\").groupBy(\"Text\").count()\n",
    "    df = df.withColumn(\"Text\", cleaner(df.Text))\n",
    "    \n",
    "    tokenizer = RegexTokenizer(inputCol=\"Text\", outputCol=\"chars\", pattern=\"\")\n",
    "    tokenized = tokenizer.transform(df)\n",
    "    vectorizer = CountVectorizer(inputCol=\"chars\", outputCol=\"features\").fit(tokenized)\n",
    "    vocab = vectorizer.vocabulary\n",
    "    chars = sorted(vocab)\n",
    "    print(chars)\n",
    "    if dict_v is None:\n",
    "        dict_v = vocab_to_dict(chars)\n",
    "        \n",
    "    labelEncoder = udf(lambda x : encode_arr(x, dict_v))\n",
    "    encoded = tokenized.withColumn(\"chars\", labelEncoder(tokenized.chars))\n",
    "    \n",
    "    listchars = [row.chars for row in encoded.collect()]\n",
    "    encoded_texts = []\n",
    "    for r in listchars:\n",
    "        for num in r:\n",
    "            if num.isnumeric():\n",
    "                encoded_texts.append(int(num))\n",
    "\n",
    "    # summarize the loaded data\n",
    "    n_chars = len(encoded_texts)\n",
    "    n_vocab = len(dict_v)\n",
    "    print(\"Total Characters: \", n_chars)\n",
    "    print(\"Total Vocab: \", n_vocab)\n",
    "    \n",
    "    # prepare the dataset of input to output pairs encoded as integers\n",
    "    seq_length = 100\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, n_chars - seq_length, 1):\n",
    "        seq_in = encoded_texts[i:i + seq_length]\n",
    "        seq_out = encoded_texts[i + seq_length]\n",
    "        dataX.append(seq_in)\n",
    "        dataY.append(seq_out)\n",
    "    n_patterns = len(dataX)\n",
    "    print(\"Total Patterns: \", n_patterns)\n",
    "    return dataX, dataY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess_candidate(df, candidate=\"Trump\")\n",
    "f = open(\"/home/ubuntu/train_trump_x.txt\", \"w\")\n",
    "for i in X:\n",
    "    f.write(str(i) +\"\\n\")\n",
    "f.close()\n",
    "\n",
    "f = open(\"/home/ubuntu/train_trump_y.txt\", \"w\")\n",
    "for i in y:\n",
    "    f.write(str(i) +\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess_candidate(df, candidate=\"Biden\")\n",
    "f = open(\"/home/ubuntu/train_biden_x.txt\", \"w\")\n",
    "for i in X:\n",
    "    f.write(str(i) +\"\\n\")\n",
    "f.close()\n",
    "\n",
    "f = open(\"/home/ubuntu/train_biden_y.txt\", \"w\")\n",
    "for i in y:\n",
    "    f.write(str(i) +\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
